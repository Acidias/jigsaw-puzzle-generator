import CryptoKit
import Foundation

/// Generates self-contained Optuna hyperparameter search scripts for Siamese networks.
/// Stateless enum - all methods are static.
enum AutoMLScriptGenerator {

    /// Compute SHA-256 hex hash of a string.
    private static func sha256Hex(_ string: String) -> String {
        let data = Data(string.utf8)
        let digest = SHA256.hash(data: data)
        return digest.map { String(format: "%02x", $0) }.joined()
    }

    /// Generate the full AutoML training script as a string.
    @MainActor
    static func generateScript(study: AutoMLStudy, datasetPath: String) -> String {
        let config = study.configuration
        let base = config.baseArchitecture

        return """
        #!/usr/bin/env python3
        \"\"\"
        AutoML Hyperparameter Search Script (Optuna)
        Generated by Jigsaw Puzzle Generator
        Study: \(study.name)
        Dataset: \(study.sourceDatasetName)
        \"\"\"

        import os
        import json
        import time
        import pandas as pd
        import numpy as np
        from PIL import Image

        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        import torch.optim as optim
        from torch.utils.data import Dataset, DataLoader
        import torchvision.transforms as transforms
        import torchvision.transforms.functional as TF
        import optuna
        from optuna.storages import RDBStorage

        # ── Fixed Configuration ───────────────────────────────────────

        DATASET_PATH = "\(datasetPath)"
        NUM_TRIALS = \(config.numTrials)
        USE_PRUNING = \(config.usePruning ? "True" : "False")
        PRUNING_STARTUP_TRIALS = \(config.pruningStartupTrials)
        OPTIMISATION_DIRECTION = "\(config.optimisationMetric.direction)"
        OPTIMISATION_METRIC = "\(config.optimisationMetric.rawValue)"

        # Fixed params from base architecture (not searched)
        \(generateFixedParams(config))

        NUM_WORKERS = min(os.cpu_count() or 1, 8 if torch.cuda.is_available() else 4)

        \(generateDeviceSelectionPython(base.devicePreference))


        # ── Dataset ────────────────────────────────────────────────────

        class SeamCropper:
            \"\"\"Crop fixed-width strips from the touching edge of each piece.\"\"\"

            def __init__(self, seam_width):
                self.seam_width = seam_width

            def __call__(self, left_img, right_img, direction):
                w, h = left_img.size
                sw = min(self.seam_width, w, h)
                if direction == "R":
                    left_strip = left_img.crop((w - sw, 0, w, h))
                    right_strip = right_img.crop((0, 0, sw, h))
                else:  # "D"
                    left_strip = left_img.crop((0, h - sw, w, h))
                    right_strip = right_img.crop((0, 0, w, sw))
                return left_strip, right_strip


        class JigsawPairDataset(Dataset):
            \"\"\"Loads jigsaw piece pairs from the dataset directory structure.\"\"\"

            def __init__(self, split_dir, transform=None, pair_augment=None, seam_cropper=None):
                self.split_dir = split_dir
                self.transform = transform
                self.pair_augment = pair_augment
                self.seam_cropper = seam_cropper
                self.pairs = []
                self.categories = []
                self.directions = []
                self.edge_info = []

                labels_path = os.path.join(split_dir, "labels.csv")
                if not os.path.exists(labels_path):
                    self._scan_directories(split_dir)
                else:
                    df = pd.read_csv(labels_path)
                    has_direction = "direction" in df.columns
                    has_edge_info = "puzzle_id" in df.columns
                    for _, row in df.iterrows():
                        left_path = os.path.join(split_dir, row["left_file"])
                        right_path = os.path.join(split_dir, row["right_file"])
                        label = int(row["label"])
                        category = row.get("category", row["left_file"].split("/")[0])
                        if os.path.exists(left_path) and os.path.exists(right_path):
                            self.pairs.append((left_path, right_path, label, category))
                            self.categories.append(category)
                            self.directions.append(row["direction"] if has_direction else "R")
                            if has_edge_info:
                                self.edge_info.append((
                                    str(row["puzzle_id"]),
                                    str(row["left_piece_id"]),
                                    row["direction"] if has_direction else "R",
                                ))
                            else:
                                self.edge_info.append(None)

            def _scan_directories(self, split_dir):
                for category in os.listdir(split_dir):
                    cat_dir = os.path.join(split_dir, category)
                    if not os.path.isdir(cat_dir):
                        continue
                    label = 1 if category == "correct" else 0
                    left_files = sorted([
                        f for f in os.listdir(cat_dir) if f.endswith("_left.png")
                    ])
                    for lf in left_files:
                        rf = lf.replace("_left.png", "_right.png")
                        left_path = os.path.join(cat_dir, lf)
                        right_path = os.path.join(cat_dir, rf)
                        if os.path.exists(right_path):
                            self.pairs.append((left_path, right_path, label, category))
                            self.categories.append(category)
                            self.directions.append("R")
                            self.edge_info.append(None)

            def __len__(self):
                return len(self.pairs)

            def __getitem__(self, idx):
                left_path, right_path, label, category = self.pairs[idx]
                left_img = Image.open(left_path).convert("RGBA")
                right_img = Image.open(right_path).convert("RGBA")

                if self.seam_cropper:
                    direction = self.directions[idx]
                    left_img, right_img = self.seam_cropper(left_img, right_img, direction)

                if self.pair_augment:
                    left_img, right_img = self.pair_augment(left_img, right_img)

                if self.transform:
                    left_img = self.transform(left_img)
                    right_img = self.transform(right_img)

                if USE_FOUR_CLASS:
                    cat_idx = CATEGORY_TO_IDX.get(category, 0)
                    return left_img, right_img, torch.tensor(cat_idx, dtype=torch.long)
                return left_img, right_img, torch.tensor(label, dtype=torch.float32)


        # ── Model ──────────────────────────────────────────────────────

        ADAPTIVE_POOL_SIZE = \(SiameseArchitecture.adaptivePoolSize)

        class SiameseNetwork(nn.Module):
            \"\"\"Siamese Neural Network with parameterised architecture.\"\"\"

            def __init__(self, num_blocks, filters_base, kernel_size, use_batch_norm,
                         embedding_dim, comparison, dropout):
                super().__init__()

                layers = []
                in_channels = 4  # RGBA
                for i in range(num_blocks):
                    out_channels = filters_base * (2 ** min(i, 4))
                    pad = kernel_size // 2
                    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=pad))
                    if use_batch_norm:
                        layers.append(nn.BatchNorm2d(out_channels))
                    layers.append(nn.ReLU())
                    layers.append(nn.MaxPool2d(2))
                    in_channels = out_channels

                self.backbone = nn.Sequential(*layers)
                self.pool = nn.AdaptiveAvgPool2d((ADAPTIVE_POOL_SIZE, ADAPTIVE_POOL_SIZE))

                flattened = in_channels * ADAPTIVE_POOL_SIZE * ADAPTIVE_POOL_SIZE
                self.embedding = nn.Sequential(
                    nn.Flatten(),
                    nn.Linear(flattened, embedding_dim),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                )

                self.comparison = comparison
                if comparison == "concat":
                    self.classifier = nn.Sequential(
                        nn.Linear(embedding_dim * 2, 128),
                        nn.ReLU(),
                        nn.Dropout(dropout),
                        nn.Linear(128, 64),
                        nn.ReLU(),
                        nn.Linear(64, NUM_CLASSES),
                    )
                else:
                    self.classifier = nn.Sequential(
                        nn.Linear(embedding_dim, 64),
                        nn.ReLU(),
                        nn.Dropout(dropout),
                        nn.Linear(64, NUM_CLASSES),
                    )

            def forward_one(self, x):
                x = self.backbone(x)
                ps = ADAPTIVE_POOL_SIZE
                pad_h = (-x.shape[2]) % ps
                pad_w = (-x.shape[3]) % ps
                if pad_h > 0 or pad_w > 0:
                    x = F.pad(x, [0, pad_w, 0, pad_h])
                x = self.pool(x)
                x = self.embedding(x)
                return x

            def forward(self, left, right):
                emb_left = self.forward_one(left)
                emb_right = self.forward_one(right)
                if self.comparison == "l1":
                    diff = torch.abs(emb_left - emb_right)
                elif self.comparison == "l2":
                    diff = (emb_left - emb_right).pow(2)
                else:
                    diff = torch.cat([emb_left, emb_right], dim=1)
                return self.classifier(diff)


        # ── Training Helpers ───────────────────────────────────────────

        class CrispAlphaResize:
            def __init__(self, size):
                self.size = (size, size) if isinstance(size, int) else size
            def __call__(self, img):
                r, g, b, a = img.split()
                rgb = Image.merge("RGB", (r, g, b))
                rgb = rgb.resize(self.size, Image.BILINEAR)
                a = a.resize(self.size, Image.NEAREST)
                r2, g2, b2 = rgb.split()
                return Image.merge("RGBA", (r2, g2, b2, a))


        class PairConsistentRGBAAugment:
            def __init__(self, brightness=0.3, contrast=0.3, saturation=0.2, grayscale_p=0.1):
                self.brightness = (max(0, 1 - brightness), 1 + brightness)
                self.contrast = (max(0, 1 - contrast), 1 + contrast)
                self.saturation = (max(0, 1 - saturation), 1 + saturation)
                self.grayscale_p = grayscale_p

            def _augment_one(self, img, fn_idx, b_factor, c_factor, s_factor, do_gray):
                r, g, b, a = img.split()
                rgb = Image.merge("RGB", (r, g, b))
                for fn_id in fn_idx:
                    if fn_id == 0: rgb = TF.adjust_brightness(rgb, b_factor)
                    elif fn_id == 1: rgb = TF.adjust_contrast(rgb, c_factor)
                    elif fn_id == 2: rgb = TF.adjust_saturation(rgb, s_factor)
                if do_gray:
                    rgb = TF.rgb_to_grayscale(rgb, num_output_channels=3)
                r2, g2, b2 = rgb.split()
                return Image.merge("RGBA", (r2, g2, b2, a))

            def __call__(self, img1, img2):
                fn_idx, b_factor, c_factor, s_factor, _ = transforms.ColorJitter.get_params(
                    brightness=self.brightness, contrast=self.contrast,
                    saturation=self.saturation, hue=None,
                )
                do_gray = torch.rand(1).item() < self.grayscale_p
                return (
                    self._augment_one(img1, fn_idx, b_factor, c_factor, s_factor, do_gray),
                    self._augment_one(img2, fn_idx, b_factor, c_factor, s_factor, do_gray),
                )


        def run_train_epoch_amp(model, loader, criterion, optimiser, scaler, use_amp):
            model.train()
            total_loss = 0.0
            correct = 0
            total = 0

            for left, right, labels in loader:
                left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)
                optimiser.zero_grad()
                with torch.amp.autocast("cuda", enabled=use_amp):
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        loss = criterion(outputs, labels)
                        preds = outputs.detach().argmax(dim=1)
                        correct += (preds == labels).sum().item()
                    else:
                        outputs = outputs.squeeze()
                        loss = criterion(outputs, labels)
                        preds = (outputs.detach() > 0.0).float()
                        correct += (preds == labels).sum().item()
                scaler.scale(loss).backward()
                scaler.step(optimiser)
                scaler.update()

                total_loss += loss.item() * labels.size(0)
                total += labels.size(0)

            return total_loss / total, correct / total


        def run_validation(model, loader, criterion, threshold=0.0):
            model.train(False)
            total_loss = 0.0
            correct = 0
            total = 0

            with torch.no_grad():
                for left, right, labels in loader:
                    left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        loss = criterion(outputs, labels)
                        preds = outputs.argmax(dim=1)
                        correct += (preds == labels).sum().item()
                    else:
                        outputs = outputs.squeeze()
                        loss = criterion(outputs, labels)
                        preds = (outputs > threshold).float()
                        correct += (preds == labels).sum().item()

                    total_loss += loss.item() * labels.size(0)
                    total += labels.size(0)

            accuracy = correct / total if total > 0 else 0
            avg_loss = total_loss / total if total > 0 else 0
            return avg_loss, accuracy


        def get_match_scores(model, loader):
            model.train(False)
            all_scores = []
            all_binary_labels = []
            with torch.no_grad():
                for left, right, labels in loader:
                    left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        probs = F.softmax(outputs, dim=1)
                        scores = probs[:, 0]
                        binary = (labels == 0).float()
                    else:
                        scores = outputs.squeeze()
                        binary = labels
                    all_scores.extend(scores.cpu().numpy())
                    all_binary_labels.extend(binary.cpu().numpy())
            return np.array(all_scores), np.array(all_binary_labels)


        def find_optimal_threshold(model, loader):
            all_scores, all_labels = get_match_scores(model, loader)
            best_threshold = 0.0
            best_f1 = 0.0
            for t in np.sort(np.unique(all_scores)):
                preds = (all_scores > t).astype(float)
                tp = ((preds == 1) & (all_labels == 1)).sum()
                fp = ((preds == 1) & (all_labels == 0)).sum()
                fn = ((preds == 0) & (all_labels == 1)).sum()
                if (tp + fp) == 0:
                    continue
                prec = tp / (tp + fp)
                rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0
                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = float(t)
            return best_threshold, best_f1


        def evaluate_standardised(model, valid_loader, test_loader, criterion,
                                  targets=(50, 60, 70, 80)):
            val_scores, val_labels = get_match_scores(model, valid_loader)
            thresholds = np.sort(np.unique(val_scores))

            target_thresholds = {}
            for target_pct in targets:
                target_prec = target_pct / 100.0
                best_recall = -1.0
                best_t = None
                for t in thresholds:
                    preds = (val_scores > t).astype(float)
                    tp = ((preds == 1) & (val_labels == 1)).sum()
                    fp = ((preds == 1) & (val_labels == 0)).sum()
                    fn = ((preds == 0) & (val_labels == 1)).sum()
                    if (tp + fp) == 0:
                        continue
                    prec = tp / (tp + fp)
                    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                    if prec >= target_prec and rec > best_recall:
                        best_recall = float(rec)
                        best_t = float(t)
                target_thresholds[target_pct] = best_t

            test_scores, test_labels = get_match_scores(model, test_loader)
            results = []
            for target_pct in targets:
                threshold = target_thresholds[target_pct]
                if threshold is None:
                    results.append({
                        "precisionTarget": target_pct, "status": "unachievable",
                        "threshold": None, "precision": None, "recall": None,
                        "accuracy": None, "f1": None, "predictedPositives": None,
                        "truePositives": None, "falsePositives": None,
                        "falseNegatives": None, "trueNegatives": None,
                    })
                    continue

                t_preds = (test_scores > threshold).astype(float)
                tp = int(((t_preds == 1) & (test_labels == 1)).sum())
                fp = int(((t_preds == 1) & (test_labels == 0)).sum())
                fn = int(((t_preds == 0) & (test_labels == 1)).sum())
                tn = int(((t_preds == 0) & (test_labels == 0)).sum())
                total_count = tp + fp + fn + tn
                test_acc = (tp + tn) / total_count if total_count > 0 else 0.0
                test_prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                test_rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                test_f1 = 2 * test_prec * test_rec / (test_prec + test_rec) if (test_prec + test_rec) > 0 else 0.0
                results.append({
                    "precisionTarget": target_pct, "status": "achieved",
                    "threshold": round(threshold, 4),
                    "precision": round(test_prec, 6), "recall": round(test_rec, 6),
                    "accuracy": round(test_acc, 6), "f1": round(test_f1, 6),
                    "predictedPositives": tp + fp, "truePositives": tp,
                    "falsePositives": fp, "falseNegatives": fn, "trueNegatives": tn,
                })
            return results


        def compute_ranking_metrics(model, test_dataset):
            model.train(False)
            loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

            all_scores = []
            all_binary_labels = []
            with torch.no_grad():
                for left, right, labels in loader:
                    left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        probs = F.softmax(outputs, dim=1)
                        scores = probs[:, 0].cpu().numpy()
                        binary = (labels == 0).float().cpu().numpy()
                    else:
                        scores = outputs.squeeze().cpu().numpy()
                        binary = labels.cpu().numpy()
                    all_scores.extend(scores)
                    all_binary_labels.extend(binary)

            all_scores = np.array(all_scores)
            all_binary_labels = np.array(all_binary_labels)

            has_edge_info = any(e is not None for e in test_dataset.edge_info)
            if has_edge_info:
                from collections import defaultdict
                edge_groups = defaultdict(list)
                for idx in range(len(test_dataset)):
                    info = test_dataset.edge_info[idx]
                    if info is None:
                        continue
                    key = info
                    edge_groups[key].append(idx)

                total_queries = 0
                pool_sizes = []
                recall_hits = {1: 0, 5: 0, 10: 0}
                for key, indices in edge_groups.items():
                    if len(indices) < 2:
                        continue
                    group_scores = all_scores[indices]
                    group_labels = all_binary_labels[indices]
                    pos_mask = group_labels == 1
                    if pos_mask.sum() == 0:
                        continue
                    pool_sizes.append(len(indices))
                    neg_scores_sorted = np.sort(group_scores[~pos_mask])[::-1]
                    for pos_idx in np.where(pos_mask)[0]:
                        total_queries += 1
                        pos_score = group_scores[pos_idx]
                        rank = int((neg_scores_sorted > pos_score).sum()) + 1
                        for k in [1, 5, 10]:
                            if rank <= k:
                                recall_hits[k] += 1

                if total_queries == 0:
                    return {"recallAt1": 0.0, "recallAt5": 0.0, "recallAt10": 0.0,
                            "edgeQueryCount": 0, "avgPoolSize": 0.0}
                results = {}
                for k in [1, 5, 10]:
                    results[f"recallAt{k}"] = round(recall_hits[k] / total_queries, 6)
                results["edgeQueryCount"] = total_queries
                results["avgPoolSize"] = round(np.mean(pool_sizes), 2)
                return results
            else:
                pos_scores = all_scores[all_binary_labels == 1]
                neg_scores = np.sort(all_scores[all_binary_labels == 0])[::-1]
                if len(pos_scores) == 0 or len(neg_scores) == 0:
                    return {"recallAt1": 0.0, "recallAt5": 0.0, "recallAt10": 0.0}
                results = {}
                for k in [1, 5, 10]:
                    hits = sum(1 for p in pos_scores if int((neg_scores > p).sum()) + 1 <= k)
                    results[f"recallAt{k}"] = round(hits / len(pos_scores), 6)
                return results


        def run_detailed_test(model, dataset, criterion, threshold=0.0):
            model.train(False)
            use_workers = DEVICE.type == "cuda"
            nw = NUM_WORKERS if use_workers else 0
            loader_kwargs = dict(num_workers=nw, pin_memory=use_workers, persistent_workers=(nw > 0))
            loader = DataLoader(dataset, batch_size=64, shuffle=False, **loader_kwargs)

            all_preds = []
            all_labels = []
            with torch.no_grad():
                for left, right, labels in loader:
                    left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        preds = outputs.argmax(dim=1)
                    else:
                        outputs = outputs.squeeze()
                        preds = (outputs > threshold).float()
                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())

            all_preds = np.array(all_preds)
            all_labels = np.array(all_labels)

            if USE_FOUR_CLASS:
                tp = int(((all_preds == 0) & (all_labels == 0)).sum())
                fp = int(((all_preds == 0) & (all_labels != 0)).sum())
                fn = int(((all_preds != 0) & (all_labels == 0)).sum())
                tn = int(((all_preds != 0) & (all_labels != 0)).sum())
            else:
                tp = int(((all_preds == 1) & (all_labels == 1)).sum())
                fp = int(((all_preds == 1) & (all_labels == 0)).sum())
                fn = int(((all_preds == 0) & (all_labels == 1)).sum())
                tn = int(((all_preds == 0) & (all_labels == 0)).sum())

            confusion_matrix = {"truePositives": tp, "falsePositives": fp,
                                "falseNegatives": fn, "trueNegatives": tn}

            per_category = {}
            for cat in sorted(set(dataset.categories)):
                mask = np.array([c == cat for c in dataset.categories])
                cat_preds = all_preds[mask]
                total_count = int(len(cat_preds))
                if USE_FOUR_CLASS:
                    predicted_match = int((cat_preds == 0).sum())
                else:
                    predicted_match = int((cat_preds == 1).sum())
                per_category[cat] = {"total": total_count, "predictedMatch": predicted_match,
                                     "predictedNonMatch": total_count - predicted_match}

            four_class_metrics = None
            if USE_FOUR_CLASS:
                n_cls = NUM_CLASSES
                cm_nxn = [[0]*n_cls for _ in range(n_cls)]
                for true_cls, pred_cls in zip(all_labels, all_preds):
                    if int(true_cls) < n_cls and int(pred_cls) < n_cls:
                        cm_nxn[int(true_cls)][int(pred_cls)] += 1
                idx_to_cat = {v: k for k, v in CATEGORY_TO_IDX.items()}
                per_class_acc = {}
                for cls_idx in range(n_cls):
                    cls_mask = all_labels == cls_idx
                    cls_total = int(cls_mask.sum())
                    cls_correct = int(((all_preds == cls_idx) & cls_mask).sum())
                    cat_name = idx_to_cat.get(cls_idx, str(cls_idx))
                    per_class_acc[cat_name] = round(cls_correct / cls_total, 6) if cls_total > 0 else 0.0
                four_class_metrics = {
                    "accuracy": round(float((all_preds == all_labels).mean()), 6),
                    "perClassAccuracy": per_class_acc,
                    "confusionMatrix4x4": cm_nxn,
                }

            return confusion_matrix, per_category, four_class_metrics


        # ── Optuna Objective ───────────────────────────────────────────

        # Preload datasets (shared across trials)
        NORM_MEAN = [0.5, 0.5, 0.5, 0.5]
        NORM_STD = [0.5, 0.5, 0.5, 0.5]


        def build_datasets(input_size, use_seam_only, seam_width, use_native_resolution):
            pair_augment = PairConsistentRGBAAugment(
                brightness=0.3, contrast=0.3, saturation=0.2, grayscale_p=0.1,
            )

            if use_seam_only:
                resize_tf = CrispAlphaResize(seam_width)
                base_transform = transforms.Compose([
                    resize_tf, transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                inference_transform = transforms.Compose([
                    resize_tf, transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                seam_cropper = SeamCropper(seam_width)
            elif use_native_resolution:
                base_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                inference_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                seam_cropper = None
            else:
                resize_tf = CrispAlphaResize(input_size)
                base_transform = transforms.Compose([
                    resize_tf, transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                inference_transform = transforms.Compose([
                    resize_tf, transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                seam_cropper = None

            train_ds = JigsawPairDataset(
                os.path.join(DATASET_PATH, "train"), base_transform, pair_augment,
                seam_cropper=seam_cropper,
            )
            valid_ds = JigsawPairDataset(
                os.path.join(DATASET_PATH, "valid"), inference_transform,
                seam_cropper=seam_cropper,
            )
            test_ds = JigsawPairDataset(
                os.path.join(DATASET_PATH, "test"), inference_transform,
                seam_cropper=seam_cropper,
            )
            return train_ds, valid_ds, test_ds


        # Cache datasets when transform params are fixed
        _cached_datasets = {}


        def get_or_build_datasets(input_size, use_seam_only, seam_width, use_native_resolution):
            key = (input_size, use_seam_only, seam_width, use_native_resolution)
            if key not in _cached_datasets:
                _cached_datasets[key] = build_datasets(input_size, use_seam_only, seam_width, use_native_resolution)
            return _cached_datasets[key]


        def objective(trial):
            trial_start = time.time()

            # ── Suggest hyperparameters ──
        \(generateSuggestBlock(config))

            # ── Build datasets ──
            train_ds, valid_ds, test_ds = get_or_build_datasets(
                input_size, use_seam_only, seam_width, use_native_resolution
            )

            use_workers = DEVICE.type == "cuda"
            nw = NUM_WORKERS if use_workers else 0
            loader_kwargs = dict(num_workers=nw, pin_memory=use_workers, persistent_workers=(nw > 0))

            if DEVICE.type == "cuda":
                torch.backends.cudnn.benchmark = True
                torch.set_float32_matmul_precision("high")

            use_amp = USE_AMP and DEVICE.type == "cuda"
            scaler = torch.amp.GradScaler("cuda", enabled=use_amp)

            train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, **loader_kwargs)
            valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, **loader_kwargs)

            # ── Build model ──
            model = SiameseNetwork(
                num_blocks=num_conv_blocks,
                filters_base=filters_base,
                kernel_size=kernel_size,
                use_batch_norm=use_batch_norm,
                embedding_dim=embedding_dimension,
                comparison=comparison_method,
                dropout=dropout,
            ).to(DEVICE)

            if USE_FOUR_CLASS:
                class_weights = torch.tensor([float(NUM_CLASSES - 1)] + [1.0] * (NUM_CLASSES - 1), device=DEVICE)
                criterion = nn.CrossEntropyLoss(weight=class_weights)
            else:
                pos_weight = torch.tensor([3.0], device=DEVICE)
                criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

            optimiser = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode="min", factor=0.5, patience=10)

            best_valid_acc = 0.0
            best_valid_loss = float("inf")

            for epoch in range(1, epochs + 1):
                train_loss, train_acc = run_train_epoch_amp(
                    model, train_loader, criterion, optimiser, scaler, use_amp,
                )
                valid_loss, valid_acc = run_validation(model, valid_loader, criterion)
                scheduler.step(valid_loss)

                if valid_acc > best_valid_acc:
                    best_valid_acc = valid_acc
                if valid_loss < best_valid_loss:
                    best_valid_loss = valid_loss
                    torch.save(model.state_dict(), f"trial_{trial.number}_best.pth")

                print(
                    f"AUTOML_EPOCH Trial {trial.number} "
                    f"Epoch {epoch}/{epochs} | "
                    f"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | "
                    f"Valid Loss: {valid_loss:.4f} Acc: {valid_acc:.4f}"
                )

                # Report for pruning
                if USE_PRUNING:
                    trial.report(valid_acc if OPTIMISATION_DIRECTION == "maximize" else valid_loss, epoch)
                    if trial.should_prune():
                        print(f"AUTOML_PRUNED Trial {trial.number} pruned at epoch {epoch}")
                        raise optuna.exceptions.TrialPruned()

            trial_duration = time.time() - trial_start

            # Determine return value based on optimisation metric
        \(generateReturnValue(config))

            print(
                f"AUTOML_TRIAL_COMPLETE {trial.number} "
                f"value={return_value:.6f} "
                f"valid_acc={best_valid_acc:.6f} "
                f"valid_loss={best_valid_loss:.6f} "
                f"duration={trial_duration:.1f}"
            )

            # Save trial result
            trial_result = {
                "trialNumber": trial.number,
                "state": "complete",
                "value": round(return_value, 6),
                "params": {k: str(v) for k, v in trial.params.items()},
                "duration": round(trial_duration, 1),
                "bestValidAccuracy": round(best_valid_acc, 6),
                "bestValidLoss": round(best_valid_loss, 6),
            }
            save_trial_result(trial_result)

            return return_value


        def save_trial_result(trial_result):
            \"\"\"Append a trial result to optuna_results.json (crash-safe).\"\"\"
            results_path = "optuna_results.json"
            if os.path.exists(results_path):
                with open(results_path) as f:
                    results = json.load(f)
            else:
                results = []

            # Replace existing trial with same number, or append
            existing_idx = next((i for i, r in enumerate(results) if r["trialNumber"] == trial_result["trialNumber"]), None)
            if existing_idx is not None:
                results[existing_idx] = trial_result
            else:
                results.append(trial_result)

            with open(results_path, "w") as f:
                json.dump(results, f, indent=2)


        # ── Main ───────────────────────────────────────────────────────

        def main():
            storage = RDBStorage(
                url="sqlite:///study.db",
                engine_kwargs={"connect_args": {"timeout": 30}},
            )

            pruner = None
            if USE_PRUNING:
                pruner = optuna.pruners.MedianPruner(
                    n_startup_trials=PRUNING_STARTUP_TRIALS,
                    n_warmup_steps=5,
                )

            study = optuna.create_study(
                study_name="\(study.name.replacingOccurrences(of: "\"", with: "\\\""))",
                storage=storage,
                direction=OPTIMISATION_DIRECTION,
                pruner=pruner,
                load_if_exists=True,
            )

            completed = len([t for t in study.trials if t.state in (
                optuna.trial.TrialState.COMPLETE, optuna.trial.TrialState.PRUNED
            )])
            remaining = max(0, NUM_TRIALS - completed)
            print(f"AUTOML_START total={NUM_TRIALS} completed={completed} remaining={remaining}")

            if remaining == 0:
                print("All trials already completed.")
            else:
                study.optimize(objective, n_trials=remaining)

            # Final results
            best = study.best_trial
            print(f"AUTOML_COMPLETE best_trial={best.number} best_value={best.value:.6f}")

            # Save best model metrics
            best_model_path = f"trial_{best.number}_best.pth"
            if os.path.exists(best_model_path):
                # Reconstruct best architecture and run full test-set assessment
                params = best.params
                num_blocks = int(params.get("num_conv_blocks", FIXED_NUM_CONV_BLOCKS))
                fb = int(params.get("filters_base", FIXED_FILTERS_BASE))
                ks = int(params.get("kernel_size", FIXED_KERNEL_SIZE))
                bn = str(params.get("use_batch_norm", FIXED_USE_BATCH_NORM)).lower() == "true"
                emb = int(params.get("embedding_dimension", FIXED_EMBEDDING_DIM))
                comp = str(params.get("comparison_method", FIXED_COMPARISON))
                do_ = float(params.get("dropout", FIXED_DROPOUT))
                bs = int(params.get("batch_size", FIXED_BATCH_SIZE))
                uso = str(params.get("use_seam_only", FIXED_USE_SEAM_ONLY)).lower() == "true"
                sw = int(params.get("seam_width", FIXED_SEAM_WIDTH))
                unr = FIXED_USE_NATIVE_RESOLUTION

                train_ds, valid_ds, test_ds = get_or_build_datasets(
                    FIXED_INPUT_SIZE, uso, sw, unr
                )

                model = SiameseNetwork(
                    num_blocks=num_blocks, filters_base=fb, kernel_size=ks,
                    use_batch_norm=bn, embedding_dim=emb, comparison=comp, dropout=do_,
                ).to(DEVICE)
                model.load_state_dict(torch.load(best_model_path, weights_only=True, map_location=DEVICE))

                use_workers = DEVICE.type == "cuda"
                nw = NUM_WORKERS if use_workers else 0
                loader_kwargs = dict(num_workers=nw, pin_memory=use_workers, persistent_workers=(nw > 0))
                valid_loader = DataLoader(valid_ds, batch_size=bs, shuffle=False, **loader_kwargs)
                test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False, **loader_kwargs)

                if USE_FOUR_CLASS:
                    class_weights = torch.tensor([float(NUM_CLASSES - 1)] + [1.0] * (NUM_CLASSES - 1), device=DEVICE)
                    criterion = nn.CrossEntropyLoss(weight=class_weights)
                else:
                    pos_weight = torch.tensor([3.0], device=DEVICE)
                    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

                opt_threshold, opt_f1 = find_optimal_threshold(model, valid_loader)
                test_loss, test_acc = run_validation(model, test_loader, criterion, threshold=opt_threshold)
                confusion_matrix, per_category, four_class_metrics = run_detailed_test(
                    model, test_ds, criterion, threshold=opt_threshold
                )
                standardised = evaluate_standardised(model, valid_loader, test_loader, criterion)
                ranking = compute_ranking_metrics(model, test_ds)

                # Compute precision/recall/F1 from confusion matrix
                tp = confusion_matrix["truePositives"]
                fp = confusion_matrix["falsePositives"]
                fn = confusion_matrix["falseNegatives"]
                test_prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                test_rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                test_f1 = 2 * test_prec * test_rec / (test_prec + test_rec) if (test_prec + test_rec) > 0 else 0.0

                best_metrics = {
                    "testLoss": round(test_loss, 6),
                    "testAccuracy": round(test_acc, 6),
                    "testPrecision": round(test_prec, 6),
                    "testRecall": round(test_rec, 6),
                    "testF1": round(test_f1, 6),
                    "bestEpoch": 0,
                    "optimalThreshold": round(opt_threshold, 6),
                    "confusionMatrix": confusion_matrix,
                    "perCategoryResults": per_category,
                    "standardisedResults": standardised,
                    "rankingMetrics": ranking,
                    "trainLoss": [],
                    "validLoss": [],
                    "trainAccuracy": [],
                    "validAccuracy": [],
                }
                if four_class_metrics is not None:
                    best_metrics["fourClassMetrics"] = four_class_metrics

                with open("best_metrics.json", "w") as f:
                    json.dump(best_metrics, f, indent=2)
                print("Saved best_metrics.json")

                # Copy best model weights
                import shutil
                shutil.copy2(best_model_path, "best_model.pth")
                print("Saved best_model.pth")

            # Clean up per-trial model files
            for f in os.listdir("."):
                if f.startswith("trial_") and f.endswith("_best.pth"):
                    try:
                        os.remove(f)
                    except OSError:
                        pass


        if __name__ == "__main__":
            main()
        """
    }

    /// Write automl_train.py + requirements.txt to a directory.
    /// Returns the SHA-256 hex hash of the generated script.
    @discardableResult
    @MainActor
    static func writeTrainingFiles(study: AutoMLStudy, datasetPath: String, to directory: URL) throws -> String {
        let fm = FileManager.default
        try fm.createDirectory(at: directory, withIntermediateDirectories: true)

        let script = generateScript(study: study, datasetPath: datasetPath)
        let scriptURL = directory.appendingPathComponent("automl_train.py")
        try script.write(to: scriptURL, atomically: true, encoding: .utf8)

        let requirements = """
        torch>=2.0
        torchvision>=0.15
        Pillow>=9.0
        pandas>=1.5
        numpy>=1.24
        coremltools>=7.0
        scikit-learn>=1.2
        optuna>=3.0
        """
        let reqURL = directory.appendingPathComponent("requirements.txt")
        try requirements.write(to: reqURL, atomically: true, encoding: .utf8)

        return sha256Hex(script)
    }

    // MARK: - Python Code Generation Helpers

    /// Generate fixed parameter constants from the base architecture.
    @MainActor
    private static func generateFixedParams(_ config: AutoMLConfiguration) -> String {
        let base = config.baseArchitecture

        var lines: [String] = []
        lines.append("FIXED_INPUT_SIZE = \(base.inputSize)")
        lines.append("FIXED_NUM_CONV_BLOCKS = \(base.convBlocks.count)")
        lines.append("FIXED_FILTERS_BASE = \(base.convBlocks.first?.filters ?? 32)")
        lines.append("FIXED_KERNEL_SIZE = \(base.convBlocks.first?.kernelSize ?? 3)")
        lines.append("FIXED_USE_BATCH_NORM = \(base.convBlocks.first?.useBatchNorm ?? true ? "True" : "False")")
        lines.append("FIXED_EMBEDDING_DIM = \(base.embeddingDimension)")
        lines.append("FIXED_COMPARISON = \"\(base.comparisonMethod.rawValue)\"")
        lines.append("FIXED_DROPOUT = \(base.dropout)")
        lines.append("FIXED_LEARNING_RATE = \(base.learningRate)")
        lines.append("FIXED_BATCH_SIZE = \(base.batchSize)")
        lines.append("FIXED_EPOCHS = \(base.epochs)")
        lines.append("FIXED_USE_FOUR_CLASS = \(base.useFourClass ? "True" : "False")")
        lines.append("FIXED_USE_SEAM_ONLY = \(base.useSeamOnly ? "True" : "False")")
        lines.append("FIXED_SEAM_WIDTH = \(base.seamWidth)")
        lines.append("FIXED_USE_NATIVE_RESOLUTION = \(base.useNativeResolution ? "True" : "False")")
        lines.append("USE_AMP = \(base.useMixedPrecision ? "True" : "False")")

        let searchedParams = Set(config.dimensions.map(\.param))
        if searchedParams.contains(.useFourClass) {
            lines.append("USE_FOUR_CLASS = FIXED_USE_FOUR_CLASS  # overridden per trial")
        } else {
            lines.append("USE_FOUR_CLASS = FIXED_USE_FOUR_CLASS")
        }
        lines.append("NUM_CLASSES = 5 if USE_FOUR_CLASS else 1")
        lines.append("CATEGORY_TO_IDX = {\"correct\": 0, \"wrong_shape_match\": 1, \"wrong_orientation\": 2, \"wrong_image_match\": 3, \"wrong_nothing\": 4}")

        return lines.joined(separator: "\n")
    }

    /// Generate the trial.suggest_*() block and fixed parameter assignments.
    @MainActor
    private static func generateSuggestBlock(_ config: AutoMLConfiguration) -> String {
        let searchedParams = Set(config.dimensions.map(\.param))
        var lines: [String] = []

        for dim in config.dimensions {
            switch dim {
            case .intRange(let param, let low, let high, let step):
                lines.append("    \(pythonVarName(param)) = trial.suggest_int(\"\(param.rawValue)\", \(low), \(high), step=\(step))")
            case .floatRange(let param, let low, let high, let log):
                lines.append("    \(pythonVarName(param)) = trial.suggest_float(\"\(param.rawValue)\", \(low), \(high), log=\(log ? "True" : "False"))")
            case .categorical(let param, let choices):
                let choiceStr = choices.map { pythonChoiceValue(param, $0) }.joined(separator: ", ")
                lines.append("    \(pythonVarName(param)) = trial.suggest_categorical(\"\(param.rawValue)\", [\(choiceStr)])")
            }
        }

        if !searchedParams.contains(.numConvBlocks) {
            lines.append("    num_conv_blocks = FIXED_NUM_CONV_BLOCKS")
        }
        if !searchedParams.contains(.filtersBase) {
            lines.append("    filters_base = FIXED_FILTERS_BASE")
        }
        if !searchedParams.contains(.kernelSize) {
            lines.append("    kernel_size = FIXED_KERNEL_SIZE")
        }
        if !searchedParams.contains(.useBatchNorm) {
            lines.append("    use_batch_norm = FIXED_USE_BATCH_NORM")
        }
        if !searchedParams.contains(.embeddingDimension) {
            lines.append("    embedding_dimension = FIXED_EMBEDDING_DIM")
        }
        if !searchedParams.contains(.comparisonMethod) {
            lines.append("    comparison_method = FIXED_COMPARISON")
        }
        if !searchedParams.contains(.dropout) {
            lines.append("    dropout = FIXED_DROPOUT")
        }
        if !searchedParams.contains(.learningRate) {
            lines.append("    learning_rate = FIXED_LEARNING_RATE")
        }
        if !searchedParams.contains(.batchSize) {
            lines.append("    batch_size = FIXED_BATCH_SIZE")
        }
        if !searchedParams.contains(.epochs) {
            lines.append("    epochs = FIXED_EPOCHS")
        }
        if !searchedParams.contains(.useSeamOnly) {
            lines.append("    use_seam_only = FIXED_USE_SEAM_ONLY")
        }
        if !searchedParams.contains(.seamWidth) {
            lines.append("    seam_width = FIXED_SEAM_WIDTH")
        }

        lines.append("    input_size = FIXED_INPUT_SIZE")
        lines.append("    use_native_resolution = FIXED_USE_NATIVE_RESOLUTION")

        return lines.joined(separator: "\n")
    }

    /// Generate the return value computation at the end of objective().
    @MainActor
    private static func generateReturnValue(_ config: AutoMLConfiguration) -> String {
        switch config.optimisationMetric {
        case .validAccuracy:
            return "    return_value = best_valid_acc"
        case .validLoss:
            return "    return_value = best_valid_loss"
        case .recallAtP70:
            return """
                # Compute R@P70 on validation set
                valid_loader_rp = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)
                test_loader_rp = DataLoader(test_ds, batch_size=batch_size, shuffle=False)
                if USE_FOUR_CLASS:
                    class_weights_rp = torch.tensor([float(NUM_CLASSES - 1)] + [1.0] * (NUM_CLASSES - 1), device=DEVICE)
                    criterion_rp = nn.CrossEntropyLoss(weight=class_weights_rp)
                else:
                    pos_weight_rp = torch.tensor([3.0], device=DEVICE)
                    criterion_rp = nn.BCEWithLogitsLoss(pos_weight=pos_weight_rp)
                std_results = evaluate_standardised(model, valid_loader_rp, test_loader_rp, criterion_rp, targets=(70,))
                r70 = std_results[0]
                recall_at_p70 = r70.get("recall", 0.0) if r70["status"] == "achieved" else 0.0
                return_value = recall_at_p70 if recall_at_p70 is not None else 0.0
            """
        }
    }

    private static func pythonVarName(_ param: SearchableParam) -> String {
        switch param {
        case .numConvBlocks: return "num_conv_blocks"
        case .filtersBase: return "filters_base"
        case .kernelSize: return "kernel_size"
        case .useBatchNorm: return "use_batch_norm"
        case .embeddingDimension: return "embedding_dimension"
        case .comparisonMethod: return "comparison_method"
        case .dropout: return "dropout"
        case .learningRate: return "learning_rate"
        case .batchSize: return "batch_size"
        case .epochs: return "epochs"
        case .useFourClass: return "use_four_class"
        case .useSeamOnly: return "use_seam_only"
        case .seamWidth: return "seam_width"
        }
    }

    private static func pythonChoiceValue(_ param: SearchableParam, _ value: String) -> String {
        switch param {
        case .useBatchNorm, .useFourClass, .useSeamOnly:
            return value == "true" ? "True" : "False"
        case .comparisonMethod:
            return "\"\(value)\""
        case .numConvBlocks, .filtersBase, .kernelSize, .embeddingDimension,
             .batchSize, .epochs, .seamWidth:
            return value
        case .dropout, .learningRate:
            return value
        }
    }

    private static func generateDeviceSelectionPython(_ preference: DevicePreference) -> String {
        switch preference {
        case .auto:
            return """
            if torch.cuda.is_available():
                DEVICE = torch.device("cuda")
            elif torch.backends.mps.is_available():
                DEVICE = torch.device("mps")
            else:
                DEVICE = torch.device("cpu")
            print(f"Using device: {DEVICE}")
            """
        case .mps:
            return """
            if torch.backends.mps.is_available():
                DEVICE = torch.device("mps")
            else:
                print("Warning: MPS not available, falling back to CPU")
                DEVICE = torch.device("cpu")
            print(f"Using device: {DEVICE}")
            """
        case .cuda:
            return """
            if torch.cuda.is_available():
                DEVICE = torch.device("cuda")
            else:
                print("Warning: CUDA not available, falling back to CPU")
                DEVICE = torch.device("cpu")
            print(f"Using device: {DEVICE}")
            """
        case .cpu:
            return """
            DEVICE = torch.device("cpu")
            print(f"Using device: {DEVICE}")
            """
        }
    }
}
