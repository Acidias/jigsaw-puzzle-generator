import CryptoKit
import Foundation

/// Generates self-contained PyTorch training scripts for Siamese networks.
/// Stateless enum - all methods are static.
enum TrainingScriptGenerator {

    /// Compute SHA-256 hex hash of a string.
    private static func sha256Hex(_ string: String) -> String {
        let data = Data(string.utf8)
        let digest = SHA256.hash(data: data)
        return digest.map { String(format: "%02x", $0) }.joined()
    }

    /// Generate the full training script as a string.
    @MainActor
    static func generateScript(model: SiameseModel, datasetPath: String = "./dataset") -> String {
        let arch = model.architecture
        return """
        #!/usr/bin/env python3
        \"\"\"
        Siamese Neural Network Training Script
        Generated by Jigsaw Puzzle Generator
        Model: \(model.name)
        Dataset: \(model.sourceDatasetName)
        \"\"\"

        import os
        import json
        import time
        import pandas as pd
        import numpy as np
        from PIL import Image

        import torch
        import torch.nn as nn
        import torch.nn.functional as F
        import torch.optim as optim
        from torch.utils.data import Dataset, DataLoader
        import torchvision.transforms as transforms
        import torchvision.transforms.functional as TF

        # ── Configuration ──────────────────────────────────────────────

        DATASET_PATH = "\(datasetPath)"
        INPUT_SIZE = \(arch.inputSize)
        BATCH_SIZE = \(arch.batchSize)
        EPOCHS = \(arch.epochs)
        LEARNING_RATE = \(arch.learningRate)
        EMBEDDING_DIM = \(arch.embeddingDimension)
        DROPOUT = \(arch.dropout)
        USE_NATIVE_RESOLUTION = \(arch.useNativeResolution ? "True" : "False")
        USE_AMP = \(arch.useMixedPrecision ? "True" : "False")
        USE_FOUR_CLASS = \(arch.useFourClass ? "True" : "False")
        USE_SEAM_ONLY = \(arch.useSeamOnly ? "True" : "False")
        SEAM_WIDTH = \(arch.seamWidth)
        NUM_CLASSES = 4 if USE_FOUR_CLASS else 1
        CATEGORY_TO_IDX = {"correct": 0, "wrong_shape_match": 1, "wrong_image_match": 2, "wrong_nothing": 3}
        NUM_WORKERS = min(os.cpu_count() or 1, \(arch.devicePreference == .cuda ? 8 : 4))

        \(generateDeviceSelectionPython(arch.devicePreference))


        # ── Dataset ────────────────────────────────────────────────────

        class SeamCropper:
            \"\"\"Crop fixed-width strips from the touching edge of each piece.\"\"\"

            def __init__(self, seam_width):
                self.seam_width = seam_width

            def __call__(self, left_img, right_img, direction):
                w, h = left_img.size
                sw = min(self.seam_width, w, h)
                if direction == "R":
                    left_strip = left_img.crop((w - sw, 0, w, h))
                    right_strip = right_img.crop((0, 0, sw, h))
                else:  # "D"
                    left_strip = left_img.crop((0, h - sw, w, h))
                    right_strip = right_img.crop((0, 0, w, sw))
                return left_strip, right_strip


        class JigsawPairDataset(Dataset):
            \"\"\"Loads jigsaw piece pairs from the dataset directory structure.\"\"\"

            def __init__(self, split_dir, transform=None, pair_augment=None, seam_cropper=None):
                self.split_dir = split_dir
                self.transform = transform
                self.pair_augment = pair_augment
                self.seam_cropper = seam_cropper
                self.pairs = []
                self.categories = []
                self.directions = []
                self.edge_info = []  # (puzzle_id, left_piece_id, direction) for per-edge ranking

                labels_path = os.path.join(split_dir, "labels.csv")
                if not os.path.exists(labels_path):
                    self._scan_directories(split_dir)
                else:
                    df = pd.read_csv(labels_path)
                    has_direction = "direction" in df.columns
                    has_edge_info = "puzzle_id" in df.columns
                    for _, row in df.iterrows():
                        left_path = os.path.join(split_dir, row["left_file"])
                        right_path = os.path.join(split_dir, row["right_file"])
                        label = int(row["label"])
                        category = row.get("category", row["left_file"].split("/")[0])
                        if os.path.exists(left_path) and os.path.exists(right_path):
                            self.pairs.append((left_path, right_path, label, category))
                            self.categories.append(category)
                            self.directions.append(row["direction"] if has_direction else "R")
                            if has_edge_info:
                                self.edge_info.append((
                                    str(row["puzzle_id"]),
                                    str(row["left_piece_id"]),
                                    row["direction"] if has_direction else "R",
                                ))
                            else:
                                self.edge_info.append(None)

            def _scan_directories(self, split_dir):
                for category in os.listdir(split_dir):
                    cat_dir = os.path.join(split_dir, category)
                    if not os.path.isdir(cat_dir):
                        continue
                    label = 1 if category == "correct" else 0
                    left_files = sorted([
                        f for f in os.listdir(cat_dir) if f.endswith("_left.png")
                    ])
                    for lf in left_files:
                        rf = lf.replace("_left.png", "_right.png")
                        left_path = os.path.join(cat_dir, lf)
                        right_path = os.path.join(cat_dir, rf)
                        if os.path.exists(right_path):
                            self.pairs.append((left_path, right_path, label, category))
                            self.categories.append(category)
                            self.directions.append("R")
                            self.edge_info.append(None)

            def __len__(self):
                return len(self.pairs)

            def __getitem__(self, idx):
                left_path, right_path, label, category = self.pairs[idx]
                left_img = Image.open(left_path).convert("RGBA")
                right_img = Image.open(right_path).convert("RGBA")

                if self.seam_cropper:
                    direction = self.directions[idx]
                    left_img, right_img = self.seam_cropper(left_img, right_img, direction)

                if self.pair_augment:
                    left_img, right_img = self.pair_augment(left_img, right_img)

                if self.transform:
                    left_img = self.transform(left_img)
                    right_img = self.transform(right_img)

                if USE_FOUR_CLASS:
                    return left_img, right_img, torch.tensor(CATEGORY_TO_IDX[category], dtype=torch.long)
                return left_img, right_img, torch.tensor(label, dtype=torch.float32)


        # ── Model ──────────────────────────────────────────────────────

        class SiameseNetwork(nn.Module):
            \"\"\"Siamese Neural Network with shared-weight CNN backbone.\"\"\"

            def __init__(self):
                super().__init__()

                # Shared CNN backbone
                layers = []
                in_channels = 4  # RGBA: RGB + alpha (piece silhouette)
        \(generateConvBlocksPython(arch.convBlocks))
                self.backbone = nn.Sequential(*layers)
                self.pool = nn.AdaptiveAvgPool2d((\(SiameseArchitecture.adaptivePoolSize), \(SiameseArchitecture.adaptivePoolSize)))

                # Embedding head
                self.embedding = nn.Sequential(
                    nn.Flatten(),
                    nn.Linear(\(arch.flattenedSize), EMBEDDING_DIM),
                    nn.ReLU(),
                    nn.Dropout(DROPOUT),
                )

        \(generateComparisonPython(arch.comparisonMethod, embeddingDim: arch.embeddingDimension))

            def forward_one(self, x):
                \"\"\"Forward pass through one branch.\"\"\"
                x = self.backbone(x)
                # Pad spatial dims to be divisible by pool output size (MPS compatibility)
                ps = \(SiameseArchitecture.adaptivePoolSize)
                pad_h = (-x.shape[2]) % ps
                pad_w = (-x.shape[3]) % ps
                if pad_h > 0 or pad_w > 0:
                    x = F.pad(x, [0, pad_w, 0, pad_h])
                x = self.pool(x)
                x = self.embedding(x)
                return x

            def forward(self, left, right):
                \"\"\"Forward pass through both branches + comparison.\"\"\"
                emb_left = self.forward_one(left)
                emb_right = self.forward_one(right)
        \(generateForwardComparisonPython(arch.comparisonMethod))


        # ── Training ───────────────────────────────────────────────────

        def run_train_epoch_amp(model, loader, criterion, optimiser, scaler, use_amp):
            model.train()
            total_loss = 0.0
            correct = 0
            total = 0

            for left, right, labels in loader:
                left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)

                optimiser.zero_grad()
                with torch.amp.autocast("cuda", enabled=use_amp):
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        loss = criterion(outputs, labels)
                        preds = outputs.detach().argmax(dim=1)
                        correct += (preds == labels).sum().item()
                    else:
                        outputs = outputs.squeeze()
                        loss = criterion(outputs, labels)
                        preds = (outputs.detach() > 0.0).float()
                        correct += (preds == labels).sum().item()
                scaler.scale(loss).backward()
                scaler.step(optimiser)
                scaler.update()

                total_loss += loss.item() * labels.size(0)
                total += labels.size(0)

            return total_loss / total, correct / total


        def run_validation(model, loader, criterion, threshold=0.0):
            model.eval()
            total_loss = 0.0
            correct = 0
            total = 0
            all_preds = []
            all_labels = []

            with torch.no_grad():
                for left, right, labels in loader:
                    left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)

                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        loss = criterion(outputs, labels)
                        preds = outputs.argmax(dim=1)
                        correct += (preds == labels).sum().item()
                        all_preds.extend(preds.cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())
                    else:
                        outputs = outputs.squeeze()
                        loss = criterion(outputs, labels)
                        preds = (outputs > threshold).float()
                        correct += (preds == labels).sum().item()
                        all_preds.extend(preds.cpu().numpy())
                        all_labels.extend(labels.cpu().numpy())

                    total_loss += loss.item() * labels.size(0)
                    total += labels.size(0)

            accuracy = correct / total if total > 0 else 0
            avg_loss = total_loss / total if total > 0 else 0

            if USE_FOUR_CLASS:
                # For 4-class: precision/recall/F1 defined on class 0 (correct) vs rest
                all_preds = np.array(all_preds)
                all_labels = np.array(all_labels)
                tp = int(((all_preds == 0) & (all_labels == 0)).sum())
                fp = int(((all_preds == 0) & (all_labels != 0)).sum())
                fn = int(((all_preds != 0) & (all_labels == 0)).sum())
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            else:
                all_preds = np.array(all_preds)
                all_labels = np.array(all_labels)
                tp = ((all_preds == 1) & (all_labels == 1)).sum()
                fp = ((all_preds == 1) & (all_labels == 0)).sum()
                fn = ((all_preds == 0) & (all_labels == 1)).sum()
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

            return avg_loss, accuracy, float(precision), float(recall), float(f1)


        def run_detailed_test(model, dataset, criterion, threshold=0.0):
            \"\"\"Detailed test with confusion matrix and per-category breakdown.\"\"\"
            model.eval()

            use_workers = DEVICE.type == "cuda"
            nw = NUM_WORKERS if use_workers else 0
            loader_kwargs = dict(
                num_workers=nw,
                pin_memory=use_workers,
                persistent_workers=(nw > 0),
            )
            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, **loader_kwargs)

            all_preds = []
            all_labels = []

            with torch.no_grad():
                for left, right, labels in loader:
                    left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        preds = outputs.argmax(dim=1)
                    else:
                        outputs = outputs.squeeze()
                        preds = (outputs > threshold).float()
                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())

            all_preds = np.array(all_preds)
            all_labels = np.array(all_labels)

            four_class_metrics = None
            if USE_FOUR_CLASS:
                # 4x4 confusion matrix
                cm_4x4 = [[0]*4 for _ in range(4)]
                for true_cls, pred_cls in zip(all_labels, all_preds):
                    cm_4x4[int(true_cls)][int(pred_cls)] += 1
                # Per-class accuracy
                idx_to_cat = {v: k for k, v in CATEGORY_TO_IDX.items()}
                per_class_acc = {}
                for cls_idx in range(4):
                    cls_mask = all_labels == cls_idx
                    cls_total = int(cls_mask.sum())
                    cls_correct = int(((all_preds == cls_idx) & cls_mask).sum())
                    cat_name = idx_to_cat.get(cls_idx, str(cls_idx))
                    per_class_acc[cat_name] = round(cls_correct / cls_total, 6) if cls_total > 0 else 0.0
                four_class_metrics = {
                    "accuracy": round(float((all_preds == all_labels).mean()), 6),
                    "perClassAccuracy": per_class_acc,
                    "confusionMatrix4x4": cm_4x4,
                }

            # Binary confusion matrix (for 4-class: class 0 = positive, rest = negative)
            if USE_FOUR_CLASS:
                tp = int(((all_preds == 0) & (all_labels == 0)).sum())
                fp = int(((all_preds == 0) & (all_labels != 0)).sum())
                fn = int(((all_preds != 0) & (all_labels == 0)).sum())
                tn = int(((all_preds != 0) & (all_labels != 0)).sum())
            else:
                tp = int(((all_preds == 1) & (all_labels == 1)).sum())
                fp = int(((all_preds == 1) & (all_labels == 0)).sum())
                fn = int(((all_preds == 0) & (all_labels == 1)).sum())
                tn = int(((all_preds == 0) & (all_labels == 0)).sum())

            confusion_matrix = {
                "truePositives": tp,
                "falsePositives": fp,
                "falseNegatives": fn,
                "trueNegatives": tn,
            }

            # Per-category breakdown
            per_category = {}
            for cat in sorted(set(dataset.categories)):
                mask = np.array([c == cat for c in dataset.categories])
                cat_preds = all_preds[mask]
                cat_labels = all_labels[mask]
                total = int(len(cat_preds))
                if USE_FOUR_CLASS:
                    predicted_match = int((cat_preds == 0).sum())
                else:
                    predicted_match = int((cat_preds == 1).sum())
                per_category[cat] = {
                    "total": total,
                    "predictedMatch": predicted_match,
                    "predictedNonMatch": total - predicted_match,
                }

            return confusion_matrix, per_category, four_class_metrics


        def get_match_scores(model, loader):
            \"\"\"Get match scores from model outputs.
            Binary: raw logits (higher = more likely match).
            4-class: P(correct) = softmax(logits)[:, 0] (higher = more likely correct).
            Returns (scores, binary_labels) where binary_labels are 1=match, 0=non-match.\"\"\"
            model.eval()
            all_scores = []
            all_binary_labels = []

            with torch.no_grad():
                for left, right, labels in loader:
                    left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        probs = F.softmax(outputs, dim=1)
                        scores = probs[:, 0]  # P(correct)
                        binary = (labels == 0).float()  # class 0 = correct = match
                    else:
                        scores = outputs.squeeze()
                        binary = labels
                    all_scores.extend(scores.cpu().numpy())
                    all_binary_labels.extend(binary.cpu().numpy())

            return np.array(all_scores), np.array(all_binary_labels)


        def find_optimal_threshold(model, loader):
            \"\"\"Sweep thresholds on the validation set to maximise F1.
            Uses sorted unique scores for exact optimality.\"\"\"
            all_scores, all_labels = get_match_scores(model, loader)

            best_threshold = 0.0
            best_f1 = 0.0

            for t in np.sort(np.unique(all_scores)):
                preds = (all_scores > t).astype(float)
                tp = ((preds == 1) & (all_labels == 1)).sum()
                fp = ((preds == 1) & (all_labels == 0)).sum()
                fn = ((preds == 0) & (all_labels == 1)).sum()
                if (tp + fp) == 0:
                    continue
                prec = tp / (tp + fp)
                rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0
                if f1 > best_f1:
                    best_f1 = f1
                    best_threshold = float(t)

            return best_threshold, best_f1


        def evaluate_standardised(model, valid_loader, test_loader, criterion,
                                  targets=(50, 60, 70, 80)):
            \"\"\"Find thresholds on validation set at fixed precision targets, then test.
            Uses match scores (logits for binary, P(correct) for 4-class).
            Targets are integer percentages (e.g. 70 = 70% precision).
            When a target is unachievable, emits status='unachievable' with null metrics.\"\"\"
            val_scores, val_labels = get_match_scores(model, valid_loader)
            thresholds = np.sort(np.unique(val_scores))

            target_thresholds = {}
            for target_pct in targets:
                target_prec = target_pct / 100.0
                best_recall = -1.0
                best_t = None
                for t in thresholds:
                    preds = (val_scores > t).astype(float)
                    tp = ((preds == 1) & (val_labels == 1)).sum()
                    fp = ((preds == 1) & (val_labels == 0)).sum()
                    fn = ((preds == 0) & (val_labels == 1)).sum()
                    if (tp + fp) == 0:
                        continue
                    prec = tp / (tp + fp)
                    rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                    if prec >= target_prec and rec > best_recall:
                        best_recall = float(rec)
                        best_t = float(t)
                target_thresholds[target_pct] = best_t

            # Apply each threshold to the test set
            test_scores, test_labels = get_match_scores(model, test_loader)
            results = []
            for target_pct in targets:
                threshold = target_thresholds[target_pct]
                if threshold is None:
                    results.append({
                        "precisionTarget": target_pct,
                        "status": "unachievable",
                        "threshold": None,
                        "precision": None,
                        "recall": None,
                        "accuracy": None,
                        "f1": None,
                        "predictedPositives": None,
                        "truePositives": None,
                        "falsePositives": None,
                        "falseNegatives": None,
                        "trueNegatives": None,
                    })
                    continue

                t_preds = (test_scores > threshold).astype(float)
                tp = int(((t_preds == 1) & (test_labels == 1)).sum())
                fp = int(((t_preds == 1) & (test_labels == 0)).sum())
                fn = int(((t_preds == 0) & (test_labels == 1)).sum())
                tn = int(((t_preds == 0) & (test_labels == 0)).sum())
                total = tp + fp + fn + tn
                test_acc = (tp + tn) / total if total > 0 else 0.0
                test_prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                test_rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                test_f1_val = 2 * test_prec * test_rec / (test_prec + test_rec) if (test_prec + test_rec) > 0 else 0.0
                results.append({
                    "precisionTarget": target_pct,
                    "status": "achieved",
                    "threshold": round(threshold, 4),
                    "precision": round(test_prec, 6),
                    "recall": round(test_rec, 6),
                    "accuracy": round(test_acc, 6),
                    "f1": round(test_f1_val, 6),
                    "predictedPositives": tp + fp,
                    "truePositives": tp,
                    "falsePositives": fp,
                    "falseNegatives": fn,
                    "trueNegatives": tn,
                })

            return results


        def compute_ranking_metrics(model, test_dataset):
            \"\"\"Compute per-edge Recall@K ranking metrics on the test set.
            Groups pairs by (puzzle_id, left_piece_id, direction) when edge info is available.
            For each edge query (correct pair), ranks it among all candidates sharing that edge.
            Falls back to global ranking when edge info is not available (old datasets).\"\"\"
            model.eval()
            loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

            all_scores = []
            all_binary_labels = []
            with torch.no_grad():
                for left, right, labels in loader:
                    left, right, labels = left.to(DEVICE), right.to(DEVICE), labels.to(DEVICE)
                    outputs = model(left, right)
                    if USE_FOUR_CLASS:
                        probs = F.softmax(outputs, dim=1)
                        scores = probs[:, 0].cpu().numpy()
                        binary = (labels == 0).float().cpu().numpy()
                    else:
                        scores = outputs.squeeze().cpu().numpy()
                        binary = labels.cpu().numpy()
                    all_scores.extend(scores)
                    all_binary_labels.extend(binary)

            all_scores = np.array(all_scores)
            all_binary_labels = np.array(all_binary_labels)

            # Check if per-edge info is available
            has_edge_info = any(e is not None for e in test_dataset.edge_info)

            if has_edge_info:
                # Per-edge ranking: group by (puzzle_id, left_piece_id, direction)
                from collections import defaultdict
                edge_groups = defaultdict(list)
                for idx in range(len(test_dataset)):
                    info = test_dataset.edge_info[idx]
                    if info is None:
                        continue
                    puzzle_id, piece_id, direction = info
                    key = (puzzle_id, piece_id, direction)
                    edge_groups[key].append(idx)

                total_queries = 0
                pool_sizes = []
                recall_hits = {1: 0, 5: 0, 10: 0}

                for key, indices in edge_groups.items():
                    if len(indices) < 2:
                        continue
                    group_scores = all_scores[indices]
                    group_labels = all_binary_labels[indices]
                    pos_mask = group_labels == 1
                    if pos_mask.sum() == 0:
                        continue

                    pool_sizes.append(len(indices))
                    neg_scores_sorted = np.sort(group_scores[~pos_mask])[::-1]

                    for pos_idx in np.where(pos_mask)[0]:
                        total_queries += 1
                        pos_score = group_scores[pos_idx]
                        rank = int((neg_scores_sorted > pos_score).sum()) + 1
                        for k in [1, 5, 10]:
                            if rank <= k:
                                recall_hits[k] += 1

                if total_queries == 0:
                    return {"recallAt1": 0.0, "recallAt5": 0.0, "recallAt10": 0.0,
                            "edgeQueryCount": 0, "avgPoolSize": 0.0}

                results = {}
                for k in [1, 5, 10]:
                    results[f"recallAt{k}"] = round(recall_hits[k] / total_queries, 6)
                results["edgeQueryCount"] = total_queries
                results["avgPoolSize"] = round(np.mean(pool_sizes), 2) if pool_sizes else 0.0
                return results

            else:
                # Global ranking fallback (old datasets without edge info)
                pos_scores = all_scores[all_binary_labels == 1]
                neg_scores = np.sort(all_scores[all_binary_labels == 0])[::-1]

                if len(pos_scores) == 0 or len(neg_scores) == 0:
                    return {"recallAt1": 0.0, "recallAt5": 0.0, "recallAt10": 0.0}

                results = {}
                for k in [1, 5, 10]:
                    hits = 0
                    for p in pos_scores:
                        rank = int((neg_scores > p).sum()) + 1
                        if rank <= k:
                            hits += 1
                    results[f"recallAt{k}"] = round(hits / len(pos_scores), 6)
                return results


        class CrispAlphaResize:
            \"\"\"Resize RGBA images with bilinear for RGB and nearest-neighbour for alpha.
            Prevents the alpha silhouette (piece geometry cue) from getting blurred
            during upscaling, while keeping smooth image content.\"\"\"

            def __init__(self, size):
                self.size = (size, size) if isinstance(size, int) else size

            def __call__(self, img):
                r, g, b, a = img.split()
                rgb = Image.merge("RGB", (r, g, b))
                rgb = rgb.resize(self.size, Image.BILINEAR)
                a = a.resize(self.size, Image.NEAREST)
                r2, g2, b2 = rgb.split()
                return Image.merge("RGBA", (r2, g2, b2, a))


        class PairConsistentRGBAAugment:
            \"\"\"Sample colour-jitter and grayscale params once, apply identically to both pieces.
            Preserves alpha channel (piece silhouette). This teaches the model that true
            neighbours share the same lighting/colour, reducing false negatives.\"\"\"

            def __init__(self, brightness=0.3, contrast=0.3, saturation=0.2, grayscale_p=0.1):
                self.brightness = (max(0, 1 - brightness), 1 + brightness)
                self.contrast = (max(0, 1 - contrast), 1 + contrast)
                self.saturation = (max(0, 1 - saturation), 1 + saturation)
                self.grayscale_p = grayscale_p

            def _augment_one(self, img, fn_idx, b_factor, c_factor, s_factor, do_gray):
                r, g, b, a = img.split()
                rgb = Image.merge("RGB", (r, g, b))
                for fn_id in fn_idx:
                    if fn_id == 0:
                        rgb = TF.adjust_brightness(rgb, b_factor)
                    elif fn_id == 1:
                        rgb = TF.adjust_contrast(rgb, c_factor)
                    elif fn_id == 2:
                        rgb = TF.adjust_saturation(rgb, s_factor)
                if do_gray:
                    rgb = TF.rgb_to_grayscale(rgb, num_output_channels=3)
                r2, g2, b2 = rgb.split()
                return Image.merge("RGBA", (r2, g2, b2, a))

            def __call__(self, img1, img2):
                fn_idx, b_factor, c_factor, s_factor, _ = transforms.ColorJitter.get_params(
                    brightness=self.brightness, contrast=self.contrast,
                    saturation=self.saturation, hue=None,
                )
                do_gray = torch.rand(1).item() < self.grayscale_p
                return (
                    self._augment_one(img1, fn_idx, b_factor, c_factor, s_factor, do_gray),
                    self._augment_one(img2, fn_idx, b_factor, c_factor, s_factor, do_gray),
                )


        def main():
            # Transforms (RGBA: 4 channels)
            NORM_MEAN = [0.5, 0.5, 0.5, 0.5]
            NORM_STD = [0.5, 0.5, 0.5, 0.5]

            # Pair-consistent augmentation: same colour jitter applied to both pieces
            pair_augment = PairConsistentRGBAAugment(
                brightness=0.3, contrast=0.3, saturation=0.2, grayscale_p=0.1,
            )

            # Build transforms based on mode
            if USE_SEAM_ONLY:
                # Seam mode: seam strips resized to square
                base_transform = transforms.Compose([
                    CrispAlphaResize(SEAM_WIDTH),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                eval_transform = transforms.Compose([
                    CrispAlphaResize(SEAM_WIDTH),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                seam_cropper = SeamCropper(SEAM_WIDTH)
            elif USE_NATIVE_RESOLUTION:
                base_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                eval_transform = transforms.Compose([
                    transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                seam_cropper = None
            else:
                base_transform = transforms.Compose([
                    CrispAlphaResize(INPUT_SIZE),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                eval_transform = transforms.Compose([
                    CrispAlphaResize(INPUT_SIZE),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=NORM_MEAN, std=NORM_STD),
                ])
                seam_cropper = None

            # Load datasets
            train_dataset = JigsawPairDataset(
                os.path.join(DATASET_PATH, "train"), base_transform, pair_augment,
                seam_cropper=seam_cropper,
            )
            valid_dataset = JigsawPairDataset(
                os.path.join(DATASET_PATH, "valid"), eval_transform,
                seam_cropper=seam_cropper,
            )
            test_dataset = JigsawPairDataset(
                os.path.join(DATASET_PATH, "test"), eval_transform,
                seam_cropper=seam_cropper,
            )

            print(f"Train: {len(train_dataset)} pairs")
            print(f"Valid: {len(valid_dataset)} pairs")
            print(f"Test:  {len(test_dataset)} pairs")

            # MPS does not benefit from worker processes (no pin_memory support,
            # and macOS fork overhead outweighs any parallelism gain).
            # CUDA benefits from workers + pinned memory for async transfers.
            use_workers = DEVICE.type == "cuda"
            nw = NUM_WORKERS if use_workers else 0
            loader_kwargs = dict(
                num_workers=nw,
                pin_memory=use_workers,
                persistent_workers=(nw > 0),
            )

            if DEVICE.type == "cuda":
                torch.backends.cudnn.benchmark = True
                torch.set_float32_matmul_precision("high")

            # AMP (Automatic Mixed Precision) for faster training on CUDA
            use_amp = USE_AMP and DEVICE.type == "cuda"
            if USE_AMP and DEVICE.type != "cuda":
                print("Warning: AMP requested but not on CUDA - disabled")
            scaler = torch.amp.GradScaler("cuda", enabled=use_amp)

            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **loader_kwargs)
            valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **loader_kwargs)
            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **loader_kwargs)

            # Model
            model = SiameseNetwork().to(DEVICE)
            if USE_FOUR_CLASS:
                # Weight class 0 (correct) higher to compensate for 1:3 imbalance
                class_weights = torch.tensor([3.0, 1.0, 1.0, 1.0], device=DEVICE)
                criterion = nn.CrossEntropyLoss(weight=class_weights)
            else:
                pos_weight = torch.tensor([3.0], device=DEVICE)
                criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
            optimiser = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimiser, mode="min", factor=0.5, patience=10
            )

            print(f"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}")
            print(f"Training for {EPOCHS} epochs...\\n")

            # Metrics storage
            metrics = {
                "trainLoss": [],
                "validLoss": [],
                "trainAccuracy": [],
                "validAccuracy": [],
            }

            best_valid_loss = float("inf")
            best_epoch = 0
            start_time = time.time()
            last_pairs_per_sec = 0.0

            for epoch in range(1, EPOCHS + 1):
                epoch_start = time.time()
                train_loss, train_acc = run_train_epoch_amp(
                    model, train_loader, criterion, optimiser, scaler, use_amp,
                )
                valid_loss, valid_acc, _, _, _ = run_validation(model, valid_loader, criterion)
                epoch_time = time.time() - epoch_start

                pairs_per_sec = len(train_loader.dataset) / epoch_time if epoch_time > 0 else 0
                last_pairs_per_sec = pairs_per_sec

                metrics["trainLoss"].append({"epoch": epoch, "value": round(train_loss, 6)})
                metrics["validLoss"].append({"epoch": epoch, "value": round(valid_loss, 6)})
                metrics["trainAccuracy"].append({"epoch": epoch, "value": round(train_acc, 6)})
                metrics["validAccuracy"].append({"epoch": epoch, "value": round(valid_acc, 6)})

                # Checkpoint best model
                if valid_loss < best_valid_loss:
                    best_valid_loss = valid_loss
                    best_epoch = epoch
                    torch.save(model.state_dict(), "best_model.pth")

                # Step LR scheduler based on validation loss
                old_lr = optimiser.param_groups[0]["lr"]
                scheduler.step(valid_loss)
                new_lr = optimiser.param_groups[0]["lr"]

                print(
                    f"Epoch {epoch:3d}/{EPOCHS} | "
                    f"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | "
                    f"Valid Loss: {valid_loss:.4f} Acc: {valid_acc:.4f}"
                    + (" *" if epoch == best_epoch else "")
                    + f" | {pairs_per_sec:.1f} pairs/s"
                )
                if new_lr < old_lr:
                    print(f"  >> LR reduced: {old_lr:.6f} -> {new_lr:.6f}")

            duration = time.time() - start_time

            # Test with best model
            model.load_state_dict(torch.load("best_model.pth", weights_only=True))

            # Find optimal decision threshold on validation set
            opt_threshold, opt_f1 = find_optimal_threshold(model, valid_loader)
            print(f"\\nOptimal threshold: {opt_threshold:.2f} (validation F1: {opt_f1:.4f})")

            # Evaluate test set using optimal threshold
            test_loss, test_acc, test_prec, test_rec, test_f1 = run_validation(
                model, test_loader, criterion, threshold=opt_threshold
            )

            # Detailed test: confusion matrix + per-category breakdown
            confusion_matrix, per_category, four_class_metrics = run_detailed_test(
                model, test_dataset, criterion, threshold=opt_threshold
            )

            print(f"\\n{'='*60}")
            print(f"Training complete in {duration:.1f}s")
            print(f"Best epoch: {best_epoch}")
            print(f"Optimal threshold: {opt_threshold:.2f}")
            print(f"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}")
            print(f"Precision: {test_prec:.4f} | Recall: {test_rec:.4f} | F1: {test_f1:.4f}")
            print(f"\\nConfusion Matrix:")
            print(f"  TP: {confusion_matrix['truePositives']}  FP: {confusion_matrix['falsePositives']}")
            print(f"  FN: {confusion_matrix['falseNegatives']}  TN: {confusion_matrix['trueNegatives']}")
            print(f"\\nPer-category results:")
            for cat, res in per_category.items():
                print(f"  {cat}: {res['predictedMatch']}/{res['total']} predicted match")

            # Standardised operating points for fair cross-model comparison
            standardised = evaluate_standardised(model, valid_loader, test_loader, criterion)
            print(f"\\nStandardised operating points (test set):")
            for r in standardised:
                if r["status"] == "unachievable":
                    print(f"  @Precision>={r['precisionTarget']}%: unachievable")
                else:
                    print(
                        f"  @Precision>={r['precisionTarget']}%: "
                        f"P={r['precision']:.3f} R={r['recall']:.3f} "
                        f"F1={r['f1']:.3f} (t={r['threshold']:.2f})"
                    )

            # 4-class metrics
            if four_class_metrics is not None:
                print(f"\\n4-Class Metrics:")
                print(f"  Overall accuracy: {four_class_metrics['accuracy']:.4f}")
                for cat_name, cat_acc in four_class_metrics['perClassAccuracy'].items():
                    print(f"  {cat_name}: {cat_acc:.4f}")

            # Ranking metrics
            ranking = compute_ranking_metrics(model, test_dataset)
            edge_info_str = ""
            if "edgeQueryCount" in ranking:
                edge_info_str = f" ({ranking['edgeQueryCount']} edges, avg {ranking.get('avgPoolSize', 0):.1f} candidates)"
            print(f"\\nRanking metrics{edge_info_str}:")
            print(f"  R@1: {ranking['recallAt1']:.4f}  R@5: {ranking['recallAt5']:.4f}  R@10: {ranking['recallAt10']:.4f}")

            # Save metrics
            metrics["testLoss"] = round(test_loss, 6)
            metrics["testAccuracy"] = round(test_acc, 6)
            metrics["testPrecision"] = round(test_prec, 6)
            metrics["testRecall"] = round(test_rec, 6)
            metrics["testF1"] = round(test_f1, 6)
            metrics["trainingDurationSeconds"] = round(duration, 2)
            metrics["bestEpoch"] = best_epoch
            metrics["optimalThreshold"] = round(opt_threshold, 6)
            metrics["confusionMatrix"] = confusion_matrix
            metrics["perCategoryResults"] = per_category
            metrics["standardisedResults"] = standardised
            metrics["rankingMetrics"] = ranking
            metrics["trainingRunInfo"] = {
                "batchSizeUsed": BATCH_SIZE,
                "ampEnabled": use_amp,
                "inputSizeUsed": SEAM_WIDTH if USE_SEAM_ONLY else INPUT_SIZE,
                "pairsPerSecond": round(last_pairs_per_sec, 2),
            }
            if four_class_metrics is not None:
                metrics["fourClassMetrics"] = four_class_metrics

            with open("metrics.json", "w") as f:
                json.dump(metrics, f, indent=2)
            print("Saved metrics.json")

            # Export to Core ML (optional - requires coremltools)
            try:
                import coremltools as ct

                model.cpu()
                trace_size = SEAM_WIDTH if USE_SEAM_ONLY else INPUT_SIZE
                example_left = torch.randn(1, 4, trace_size, trace_size)
                example_right = torch.randn(1, 4, trace_size, trace_size)
                traced = torch.jit.trace(model, (example_left, example_right))

                ml_model = ct.convert(
                    traced,
                    inputs=[
                        ct.TensorType(name="left", shape=(1, 4, trace_size, trace_size)),
                        ct.TensorType(name="right", shape=(1, 4, trace_size, trace_size)),
                    ],
                )
                ml_model.save("model.mlpackage")
                print("Saved model.mlpackage")
            except ImportError:
                print("coremltools not installed - skipping Core ML export")
            except Exception as e:
                print(f"Core ML export failed: {e}")


        if __name__ == "__main__":
            main()
        """
    }

    /// Export a complete training package (script + requirements + optionally the dataset).
    /// Returns the SHA-256 hex hash of the generated training script.
    @discardableResult
    @MainActor
    static func exportTrainingPackage(
        model: SiameseModel,
        dataset: PuzzleDataset,
        to destination: URL,
        includeDataset: Bool = true
    ) throws -> String {
        let fm = FileManager.default
        try fm.createDirectory(at: destination, withIntermediateDirectories: true)

        // Write train.py
        let datasetRelPath = includeDataset ? "./dataset" : "<path-to-dataset>"
        let script = generateScript(model: model, datasetPath: datasetRelPath)
        let scriptURL = destination.appendingPathComponent("train.py")
        try script.write(to: scriptURL, atomically: true, encoding: .utf8)

        // Write requirements.txt
        let requirements = """
        torch>=2.0
        torchvision>=0.15
        Pillow>=9.0
        pandas>=1.5
        numpy>=1.24
        coremltools>=7.0
        scikit-learn>=1.2
        """
        let reqURL = destination.appendingPathComponent("requirements.txt")
        try requirements.write(to: reqURL, atomically: true, encoding: .utf8)

        // Optionally copy dataset
        if includeDataset {
            let datasetDest = destination.appendingPathComponent("dataset")
            try DatasetStore.exportDataset(dataset, to: datasetDest)
        }

        return sha256Hex(script)
    }

    /// Write just train.py + requirements.txt to a directory (for TrainingRunner).
    /// The script points directly at the given dataset path - no dataset copying.
    /// Returns the SHA-256 hex hash of the generated training script.
    @discardableResult
    @MainActor
    static func writeTrainingFiles(model: SiameseModel, datasetPath: String, to directory: URL) throws -> String {
        let fm = FileManager.default
        try fm.createDirectory(at: directory, withIntermediateDirectories: true)

        let script = generateScript(model: model, datasetPath: datasetPath)
        let scriptURL = directory.appendingPathComponent("train.py")
        try script.write(to: scriptURL, atomically: true, encoding: .utf8)

        let requirements = """
        torch>=2.0
        torchvision>=0.15
        Pillow>=9.0
        pandas>=1.5
        numpy>=1.24
        coremltools>=7.0
        scikit-learn>=1.2
        """
        let reqURL = directory.appendingPathComponent("requirements.txt")
        try requirements.write(to: reqURL, atomically: true, encoding: .utf8)

        return sha256Hex(script)
    }

    // MARK: - Python Code Generation Helpers

    private static func generateConvBlocksPython(_ blocks: [ConvBlock]) -> String {
        var lines: [String] = []
        for (i, block) in blocks.enumerated() {
            let pad = block.kernelSize / 2
            lines.append("        # Conv block \(i + 1)")
            lines.append("        layers.append(nn.Conv2d(in_channels, \(block.filters), kernel_size=\(block.kernelSize), padding=\(pad)))")
            if block.useBatchNorm {
                lines.append("        layers.append(nn.BatchNorm2d(\(block.filters)))")
            }
            lines.append("        layers.append(nn.ReLU())")
            if block.useMaxPool {
                lines.append("        layers.append(nn.MaxPool2d(2))")
            }
            lines.append("        in_channels = \(block.filters)")
            if i < blocks.count - 1 {
                lines.append("")
            }
        }
        return lines.joined(separator: "\n")
    }

    private static func generateComparisonPython(_ method: ComparisonMethod, embeddingDim: Int) -> String {
        switch method {
        case .l1Distance, .l2Distance:
            return """
                    # Classification head (NUM_CLASSES=4 for 4-class, 1 for binary)
                    self.classifier = nn.Sequential(
                        nn.Linear(EMBEDDING_DIM, 64),
                        nn.ReLU(),
                        nn.Dropout(DROPOUT),
                        nn.Linear(64, NUM_CLASSES),
                    )
            """
        case .concatenation:
            return """
                    # Classification head (NUM_CLASSES=4 for 4-class, 1 for binary)
                    self.classifier = nn.Sequential(
                        nn.Linear(EMBEDDING_DIM * 2, 128),
                        nn.ReLU(),
                        nn.Dropout(DROPOUT),
                        nn.Linear(128, 64),
                        nn.ReLU(),
                        nn.Linear(64, NUM_CLASSES),
                    )
            """
        }
    }

    private static func generateDeviceSelectionPython(_ preference: DevicePreference) -> String {
        switch preference {
        case .auto:
            return """
            if torch.cuda.is_available():
                DEVICE = torch.device("cuda")
            elif torch.backends.mps.is_available():
                DEVICE = torch.device("mps")
            else:
                DEVICE = torch.device("cpu")
            print(f"Using device: {DEVICE}")
            """
        case .mps:
            return """
            if torch.backends.mps.is_available():
                DEVICE = torch.device("mps")
            else:
                print("Warning: MPS not available, falling back to CPU")
                DEVICE = torch.device("cpu")
            print(f"Using device: {DEVICE}")
            """
        case .cuda:
            return """
            if torch.cuda.is_available():
                DEVICE = torch.device("cuda")
            else:
                print("Warning: CUDA not available, falling back to CPU")
                DEVICE = torch.device("cpu")
            print(f"Using device: {DEVICE}")
            """
        case .cpu:
            return """
            DEVICE = torch.device("cpu")
            print(f"Using device: {DEVICE}")
            """
        }
    }

    private static func generateForwardComparisonPython(_ method: ComparisonMethod) -> String {
        switch method {
        case .l1Distance:
            return """
                    distance = torch.abs(emb_left - emb_right)
                    return self.classifier(distance)
            """
        case .l2Distance:
            return """
                    distance = (emb_left - emb_right).pow(2)
                    return self.classifier(distance)
            """
        case .concatenation:
            return """
                    combined = torch.cat([emb_left, emb_right], dim=1)
                    return self.classifier(combined)
            """
        }
    }
}
